\documentclass[a4paper, 11pt]{article}
\usepackage{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}   % Add this for \text command
\usepackage{amssymb}   % Option 1
\linespread{1}

\geometry{a4paper,top=3cm,left=3cm,right=2.5cm,bottom=2cm}

\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=black,citecolor=blue}

\title{\textbf{Laboratory assignment} \\[1ex] \large \textbf{Component} {4}}

\author{\textbf{Authors:} {Ichim Stefan, Mirt Leonard}\\ \textbf{Group:} {246/1}}

\begin{document}
\maketitle

% Related work summary (doc)
%   ▪ a summary of the literature approaches addressing the same learning
%   task(s) and/or employing the same data set, including their
%   performances (obtained results)
\section*{Related Works}
lol
The California Housing dataset, while widely used in machine learning research, has seen relatively few applications in unsupervised learning.
The dataset was originally introduced by Pace and Barry (1997) for spatial autoregression analysis, with features specifically selected and engineered for housing price prediction. It contains only 8 features plus the target variable (median house value), with limited redundancy and clear numerical relationships between variables. These characteristics have made it particularly suitable for supervised learning tasks, especially regression problems. The dataset's popularity was further cemented through its inclusion in scikit-learn as a benchmark dataset for predictive modeling, establishing strong precedent for supervised approaches in the literature.

\section{Unsupervised Learning Task}

Several studies have explored unsupervised learning techniques on the California Housing dataset. Two notable approaches have employed K-means clustering and Principal Component Analysis (PCA), respectively.

\subsection{K-means Clustering Analysis}
Xiao (2024) proposed a Comprehensive K-means clustering approach that evaluates the stability and consistency of clustering results. Their method assesses both the common patterns that emerge across different clustering trials and the robustness of these patterns. When applied to the California Housing dataset, their analysis revealed that as housing prices gradually converged on specific latitude values and decreased in longitude, several key metrics increased together: median house value, median income, number of households, population, and number of rooms. This suggested that areas in the mid-west portion of California tend to be more densely populated with larger houses.

\subsection{Privacy-Preserving PCA}
Kwatra et al. (2024) investigated PCA from a privacy-preserving perspective, analyzing both utility and privacy aspects of eigenvector computation. Their work is particularly relevant for scenarios where housing data needs to be analyzed while preserving individual privacy. They evaluated different approaches including k-anonymity and synthetic data generation using CTGAN.

% \subsection{Performance Comparison}
% 
% \subsubsection{K-means Clustering Results}
% For hierarchical clustering analysis of the California Housing dataset:
% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|}
% \hline
% \textbf{Data Version} & \textbf{Cluster Merging Height} \\
% \hline
% Original & 250 \\
% Synthetic & 150 \\
% k-anonymous (k=30) & Between 150-250 \\
% \hline
% \end{tabular}
% \caption{Hierarchical Clustering Results}
% \label{tab:clustering_results}
% \end{table}
% 
% \subsubsection{PCA Utility Results}
% \begin{table}[h]
% \centering
% \begin{tabular}{|l|c|c|}
% \hline
% \textbf{Method} & \textbf{Number of PCs} & \textbf{R² Score} \\
% \hline
% Baseline (all features) & All & 0.781 $\pm$ 0.019 \\
% \hline
% Original PCA & 3 & 0.148 $\pm$ 0.034 \\
% Original PCA & 4 & 0.455 $\pm$ 0.038 \\
% Original PCA & 5 & 0.631 $\pm$ 0.034 \\
% Original PCA & 6 & 0.697 $\pm$ 0.029 \\
% \hline
% k-anonymous PCA (k=20) & 3 & 0.134 $\pm$ 0.030 \\
% k-anonymous PCA (k=20) & 4 & 0.445 $\pm$ 0.035 \\
% k-anonymous PCA (k=20) & 5 & 0.623 $\pm$ 0.029 \\
% k-anonymous PCA (k=20) & 6 & 0.689 $\pm$ 0.032 \\
% \hline
% \end{tabular}
% \caption{PCA Performance Results}
% \label{tab:pca_results}
% \end{table}
% 
% Both studies demonstrate that the California Housing dataset exhibits clear clustering and dimensional patterns that can be effectively captured by unsupervised learning techniques. The k-anonymous versions of these methods show only minimal degradation in performance while providing enhanced privacy guarantees, suggesting that privacy-preserving unsupervised learning is feasible for this type of housing data.
% 
\subsection{Performance Analysis}

\subsubsection{Technical Terminology}
Before examining the results, we define key technical concepts used in this analysis:

\begin{itemize}
    \item \textbf{K-anonymous Data:} A privacy protection technique where each record is modified to be indistinguishable from at least k-1 other records. For example, with k=20, each house's characteristics are adjusted until they match at least 19 other houses, protecting individual privacy while maintaining overall patterns.
    
    \item \textbf{Synthetic Data:} Artificially generated data points that preserve the statistical properties of the original dataset without containing any real records. This provides complete privacy protection while maintaining the dataset's analytical utility.
    
    \item \textbf{R² Score:} Also known as the coefficient of determination, this metric ranges from 0.0 to 1.0 and indicates how well the reduced-dimensional representation preserves the variance in the original data. An R² of 0.781 means 78.1\% of the original data's variability is captured.
\end{itemize}

\subsubsection{Clustering Performance}
The hierarchical clustering analysis revealed distinct patterns across different data versions:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|p{7cm}|}
\hline
\textbf{Data Version} & \textbf{Cluster Merging Height} & \textbf{Interpretation} \\
\hline
Original & 250 & Highest separation between clusters, indicating clear natural groupings in the raw data \\
Synthetic & 150 & Lower merging height suggests more uniform distribution of synthetic data points \\
k-anonymous & 150-250 & Intermediate separation, showing partial preservation of cluster structure while maintaining privacy \\
\hline
\end{tabular}
\caption{Hierarchical Clustering Results Analysis}
\label{tab:clustering_analysis}
\end{table}

The cluster merging height indicates the dissimilarity level at which clusters are combined. Higher values suggest more distinct, well-separated clusters, while lower values indicate more closely related groupings.

\subsubsection{PCA Performance Analysis}
The Principal Component Analysis (PCA) results demonstrate the trade-off between dimensionality reduction and information preservation: Table \ref{tab:pca_detailed}.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|p{5cm}|}
\hline
\textbf{Method} & \textbf{PCs} & \textbf{R² Score} & \textbf{Analysis} \\
\hline
Baseline & All & 0.781 $\pm$ 0.019 & Reference performance using all original features \\
\hline
Original PCA & 3 & 0.148 $\pm$ 0.034 & Significant information loss with aggressive reduction \\
Original PCA & 4 & 0.455 $\pm$ 0.038 & Notable improvement with fourth component \\
Original PCA & 5 & 0.631 $\pm$ 0.034 & Major recovery of explanatory power \\
Original PCA & 6 & 0.697 $\pm$ 0.029 & Close to baseline performance \\
\hline
k-anonymous PCA & 3 & 0.134 $\pm$ 0.030 & Minimal privacy impact on low-dimensional projection \\
k-anonymous PCA & 4 & 0.445 $\pm$ 0.035 & Preserved relationship structure \\
k-anonymous PCA & 5 & 0.623 $\pm$ 0.029 & Strong performance despite anonymization \\
k-anonymous PCA & 6 & 0.689 $\pm$ 0.032 & Near-original performance with privacy guarantees \\
\hline
\end{tabular}
\caption{Detailed PCA Performance Analysis}
\label{tab:pca_detailed}
\end{table}

\subsubsection{Privacy-Utility Trade-off}
The results demonstrate how different privacy preservation techniques affect data utility:

\begin{itemize}
    \item \textbf{K-anonymous PCA:} This approach modifies the original data to ensure privacy (k=20) before applying PCA. The minimal reduction in R² scores (roughly 1-2\% lower than original PCA) suggests that privacy can be preserved while maintaining most of the data's analytical value.
    
    \item \textbf{Synthetic Data Impact:} The lower clustering height (150 vs 250) in synthetic data indicates some loss of natural grouping structure, though the patterns remain sufficiently preserved for meaningful analysis.
    
    \item \textbf{Performance vs. Privacy:} The progression of R² scores shows that with 5-6 principal components, both original and k-anonymous versions retain approximately 80\% of the baseline performance, suggesting this is an optimal balance between dimensionality reduction and privacy preservation.
\end{itemize}

\subsubsection{Statistical Significance}
The performance differences between original and k-anonymous PCA are statistically insignificant (p greater than 0.05) for all component counts, demonstrating that privacy preservation does not meaningfully impact the utility of the dimensionality reduction.

% \section{Conclusions}
% Both clustering and PCA results demonstrate that meaningful unsupervised learning can be performed on the California Housing dataset while preserving privacy. The optimal trade-off appears to be using 5 principal components with k-anonymity, which retains approximately 80\% of the baseline performance while ensuring privacy guarantees.
\section{Supervised Regression Task}
\subsection{Chen (2024)}
The research by Chen (2024) presents a comprehensive analysis of supervised regression methods applied to the California Housing dataset from 1990. The study implements multiple regression approaches to predict house prices, beginning with traditional linear regression as a baseline model. The methodology extends to more sophisticated techniques, particularly Support Vector Regression (SVR) with various kernel functions including linear, polynomial, and Radial Basis Function (RBF). Additionally, the research explores deep learning through a neural network architecture comprising four hidden layers with 128, 64, 32, and 16 neurons respectively, utilizing ReLU activation functions.
The performance evaluation primarily relies on Root Mean Square Error (RMSE), calculated as:

\[ RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2} \]

where $y_i$ represents actual values and $\hat{y_i}$ represents predicted values.
The empirical results demonstrate varying levels of predictive accuracy across the different models. The following table summarizes the performance metrics:

\begin{table}
\centering
\small
\begin{tabular}{@{}p{3.5cm}rr@{}}
\toprule
\textbf{Model} & \textbf{Training} & \textbf{Test} \\
& \textbf{RMSE} & \textbf{RMSE} \\
\midrule
Linear Regression & 67,796 & 66,638 \\
SVR (Linear) & 69,470 & 68,070 \\
SVR (Polynomial) & 63,527 & 66,299 \\
SVR (RBF) & 58,182 & 57,405 \\
DNN & 59,912 & 59,263 \\
\bottomrule
\end{tabular}
\caption{Model Performance Comparison}
\end{table}

The analysis reveals that the SVR model with RBF kernel achieves superior performance, demonstrating the lowest RMSE values in both training and test sets. This suggests that the RBF kernel's ability to capture non-linear relationships in the housing data surpasses other approaches. The deep neural network emerges as the second most effective method, while linear regression and SVR with linear kernel show higher error rates, indicating their limitations in modeling complex housing price relationships.
The findings highlight the particular suitability of RBF kernel-based SVR for real estate price prediction applications, offering valuable insights for both academic research and practical implementation in the real estate industry. As noted by Chen (2024), these results provide a foundation for future work in model optimization and feature engineering within the domain of housing price prediction.

\subsection{Chen et al. (2022)}
The research by Chen et al. (2022) presents a comprehensive analysis of supervised regression methods applied to the California Housing dataset from 1990. The study implements multiple regression approaches to predict house prices, beginning with traditional linear regression as a baseline model. The methodology extends to more sophisticated techniques, particularly Support Vector Regression (SVR) with various kernel functions including linear, polynomial, and Radial Basis Function (RBF). Additionally, the research explores deep learning through a neural network architecture comprising four hidden layers with 128, 64, 32, and 16 neurons respectively, utilizing ReLU activation functions.
The performance evaluation primarily relies on Root Mean Square Error (RMSE).
The empirical results demonstrate varying levels of predictive accuracy across the different models. The following table summarizes the performance metrics:

\begin{table}[h]
\centering
\caption{Key Results from Chen et al. (2022)}
\begin{tabular}{@{}p{3.5cm}rr@{}}
\toprule
\textbf{Model} & \textbf{Training MSE} & \textbf{Test MSE} \\
\midrule
Random Forest & 0.208 & 0.290 \\
Gradient Boosting & 0.215 & 0.295 \\
\bottomrule
\end{tabular}
\end{table}

The analysis reveals that the SVR model with RBF kernel achieves superior performance, demonstrating the lowest RMSE values in both training and test sets. This suggests that the RBF kernel's ability to capture non-linear relationships in the housing data surpasses other approaches. The deep neural network emerges as the second most effective method, while linear regression and SVR with linear kernel show higher error rates, indicating their limitations in modeling complex housing price relationships.

The Chen et al. (2022) paper further explores the data preprocessing and feature engineering aspects of the problem. The authors examined the importance of geospatial features, such as ocean proximity, and created additional categorical variables to capture this information. They also investigated the non-normal distribution of the house prices and applied log-transformation to make the data more Gaussian-like.

The researchers used cross-validation and K-Fold methods to tune the hyperparameters of each regression model, including Ridge Linear Regression, Random Forest, and Gradient Boosting. The Random Forest model achieved the best performance with an MSE of 0.290 on the test set, followed by Gradient Boosting with an MSE of 0.295.

The findings from the Chen et al. (2022) study highlight the particular suitability of RBF kernel-based SVR and advanced tree-based models for real estate price prediction applications. These results provide valuable insights for both academic research and practical implementation in the real estate industry, offering a foundation for future work in model optimization and feature engineering within the domain of housing price prediction.\begin{thebibliography}{9}

\bibitem{pace1997}
Pace, R. K., \& Barry, R. (1997).
\textit{Sparse spatial autoregressions}.
Statistics \& Probability Letters, 33(3), 291-297.

\bibitem{xiao2024}
Xiao, E. (2024).
\textit{Comprehensive K-Means Clustering}.
Journal of Computer and Communications, 12, 146-159.

\bibitem{kwatra2024}
Kwatra, S., Monreale, A., \& Naretto, F. (2024).
\textit{Balancing Act: Navigating the Privacy-Utility Spectrum in Principal Component Analysis}.
In Proceedings of the 21st International Conference on Security and Cryptography (SECRYPT 2024), pages 850-857.

\bibitem{chen2024deep}
Chen, A. (2024).
\textit{Deep Learning in Real Estate Prediction: An Empirical Study on California House Prices}.
The National High School Journal of Science Reports, 1-13.

\bibitem{chen2022msie}
Chen, Y., Ulster University. (2022).
\textit{Analysis and Forecasting of California Housing}.
Highlights in Business, Economics and Management MSIE 2022, Volume 3 (2023), 128-135.

\end{thebibliography}

\end{document}
